# -*- coding: utf-8 -*-
"""BestGomoku.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UUh_yRw_iOZk3oPxaAnbLdQLbx9Cm0YA

# BestGomoku Training
This is the file  that contains all of the logic of the BestGomoku agent, using reinforcement learning to teach itself how to play Gomoku by playing against a large number of games.

Our model implementation took a lot of inspiration from the Deep Q Learning Agent implementated for the [Blob Game](https://pythonprogramming.net/deep-q-learning-dqn-reinforcement-learning-python-tutorial/), and our training implementation took a lot from how the Blob Agent was trained (found [here](https://pythonprogramming.net/training-deep-q-learning-dqn-reinforcement-learning-python-tutorial/?completed=/deep-q-learning-dqn-reinforcement-learning-python-tutorial/)). 

Where our work differs is in the specifics for Gomoku; the model created at the tutorial was a basic model that focused on a rather simple game, so our implementation completely differs in how it is geared towards the game of Gomoku. We also changed the Q-Learning approximation model architecture quite a bit,

## Setup

### Imports
"""

import pandas as pd
import numpy as np
import os
import gym
import time
import random
from tqdm import tqdm
from collections import deque
from gym import spaces

import tensorflow as tf
from keras import Sequential
from keras.layers import Conv1D, Activation, MaxPooling1D, Dropout, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from keras.callbacks import TensorBoard

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

import gym_gomoku
from gym.envs.registration import register

"""### Create new OpenAI Gym environments
This will be two environments, namely two 15x15 boards with one making random moves, and the other making good moves.
"""

# Register new 15x15 environment
register(
    id='Gomoku15x15-v1',
    entry_point='gym_gomoku.envs:GomokuEnv',
    kwargs={
        'player_color': 'black',
        'opponent': 'beginner', # beginner opponent policy has defend and strike rules
        'board_size': 15,
    },
    nondeterministic=True,
)

# Register Random Gomoku Env
register(
    id='Gomoku15x15-v0',
    entry_point='gym_gomoku.envs:GomokuEnv',
    kwargs={
        'player_color': 'black',
        'opponent': 'random',
        'board_size': 15,
    },
    nondeterministic=True,
)

ENV_RANDOM = False

if ENV_RANDOM:
  env = gym.make('Gomoku15x15-v0')
else:
  env = gym.make('Gomoku15x15-v1')

# Set Play to true to experience a game
play = False

if play:
  # Playing around with the Gomoku Env
  env.reset()
  env.render('human')
  env.step(15) # place a single stone, black color first

  # play a game
  env.reset()
  for _ in range(20):
      action = env.action_space.sample() # sample without replacement
      observation, reward, done, info = env.step(action)
      env.render('human')
      if done:
          print ("Game is Over")
          break

"""### Other Setup
This includes defining the random seeds to reproduce similar data, and cloning our github to access our training results.
"""

# For more repetitive results
random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

# Memory fraction, used mostly when trai8ning multiple agents
#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)
#backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))

# Create models folder
if not os.path.isdir('models'):
    os.makedirs('models')


"""## Model Implementation
This section contains the implementation of a tensorboard custom class used for plotting the data, and our Deep Q-Learning model implementation.

### Making Custom Heuristic for Gomoku

We decided to define our own heuristic to aid our model during training.
"""

def find_length_in_row(row, player):
  counts = [0, 0, 0, 0, 0]
  i = 0
  while i < len(row):
    curr_count = 0
    if row[i] == player:
      j = i + 1
      while j < len(row) and row[j] == player:
        curr_count += 1
        j += 1
      
      counts[curr_count] += counts[curr_count] + 1
      i = j - 1
    
    i += 1
  return counts[1:]

def find_score(board, player):
  # This is [num_2_components, num_3_components, ...]
  # Note that we are not counting number of 1 components as that is not helpful

  # A LOT OF BOOKKEEPING
  row_board = np.array(board)
  col_board = row_board.T 

  diags = [row_board[::-1,:].diagonal(i) for i in range(-row_board.shape[0]+1,row_board.shape[1])]
  diags.extend(row_board.diagonal(i) for i in range(row_board.shape[1]-1,-row_board.shape[0],-1))
  r_diag_board = [n.tolist() for n in diags][1:-1]

  diags = [col_board[::-1,:].diagonal(i) for i in range(-col_board.shape[0]+1,col_board.shape[1])]
  diags.extend(col_board.diagonal(i) for i in range(col_board.shape[1]-1,-col_board.shape[0],-1))
  l_diag_board = [n.tolist() for n in diags][1:-1]

  rows = [find_length_in_row(n, player) for n in row_board]
  cols = [find_length_in_row(n, player) for n in col_board]
  ldiag = [find_length_in_row(n, player) for n in l_diag_board]
  rdiag = [find_length_in_row(n, player) for n in r_diag_board]

  # Sum all scores in all counts
  counts = [0, 0, 0, 0]

  for a, b, c, d in rows:
    counts[0] += a
    counts[1] += b
    counts[2] += c
    counts[3] += d
  
  for a, b, c, d in cols:
    counts[0] += a
    counts[1] += b
    counts[2] += c
    counts[3] += d

  for a, b, c, d in ldiag:
    counts[0] += a
    counts[1] += b
    counts[2] += c
    counts[3] += d
  
  for a, b, c, d in rdiag:
    counts[0] += a
    counts[1] += b
    counts[2] += c
    counts[3] += d
  
  return counts

def find_reward(env, color, mults, opponent_mult):
  player_color = env.player_color
  player = 1 if player_color is 'black' else 2
  state = env.state.board.board_state

  player_reward = find_score(state, player)
  computer_reward = find_score(state, 2 if player_color is 'black' else 1)

  # If player has won, just return 100,000
  if player_reward[3] > 0:
    return 100000
  
  elif computer_reward[3] > 0:
    return -100000

  player_sum = sum([(player_reward[i] * mults[i]) for i in range(len(player_reward))])
  computer_sum = sum([(computer_reward[i] * mults[i] * opponent_mult) for i in range(len(computer_reward))])


  return player_sum - computer_sum

"""### Define Custom function for finding maximum Q value in a list"""

def find_max_Q(qs, valid_spaces):
  max_val = qs[valid_spaces[0]]
  max_idx = valid_spaces[0]
  for i in valid_spaces[1:]:
    if qs[i] > max_val:
      max_val = qs[i]
      max_idx = i
  
  return max_idx

"""### Create Modified Tensorboard class"""

class ModifiedTensorBoard(TensorBoard):     
                                        
  def __init__(self, **kwargs):
      super().__init__(**kwargs)
      self.step = 1
      self.writer = tf.summary.create_file_writer(self.log_dir)
      self._log_write_dir = self.log_dir

  def set_model(self, model):
      self.model = model

      self._train_dir = os.path.join(self._log_write_dir, 'train')
      self._train_step = self.model._train_counter

      self._val_dir = os.path.join(self._log_write_dir, 'validation')
      self._val_step = self.model._test_counter

      self._should_write_train_graph = False

  def on_epoch_end(self, epoch, logs=None):
      self.update_stats(**logs)

  def on_batch_end(self, batch, logs=None):
      pass

  def on_train_end(self, _):
      pass

  def update_stats(self, **stats):
      with self.writer.as_default():
          for key, value in stats.items():
              tf.summary.scalar(key, value, step = self.step)
              self.writer.flush()

"""### Create the Deep Q-Learning Agent"""

DISCOUNT = 0.99
REPLAY_MEMORY_SIZE = 10_000
MIN_REPLAY_MEMORY_SIZE = 1_000
MIN_REWARD = -5000  # For model save
MEMORY_FRACTION = 0.20
MINIBATCH_SIZE = 64
UPDATE_TARGET_EVERY = 5 
BOARD_SIZE = 15

from keras.layers.convolutional import Conv1D
class DQNAgent:
  def __init__(self, model_location=None, model_weights=None):
    # Initialize random Q Model
    if model_location == None:
      self.model = self.create_model()

    # Using pretrained model
    else:
      self.model = tf.keras.models.load_model(model_location)
    
    if model_weights != None:
      self.model.load_weights(model_weights)

    # Our Target Q Model
    self.target_model = self.create_model()
    self.target_model.set_weights(self.model.get_weights())


    # Our Replay Memory which stores previous actions
    self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)

    # Custom tensorboard object
    self.tensorboard = ModifiedTensorBoard(log_dir="logs/{}-{}".format(MODEL_NAME, int(time.time())))

    # Used to count when to update target network with main network's weights
    self.target_update_counter = 0
  
  def create_model(self):
    model = tf.keras.Sequential()
    model.add(Flatten(input_shape=(env.board_size, env.board_size)))
    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dropout(0.2))

    model.add(Dense(512))
    model.add(Activation('relu'))
    model.add(Dropout(0.2))

    model.add(Dense(512))

    model.add(Dense(env.board_size**2, activation='linear'))  
    model.compile(loss="mse", optimizer=Adam(lr=0.001), metrics=['accuracy'])
    return model

  # Adds step's data to a memory replay array
  # (observation space, action, reward, new observation space, done)
  def update_replay_memory(self, transition):
    self.replay_memory.append(transition)
  
  # Trains main network every step during episode
  def train(self, terminal_state, step):
    # Start training only if certain number of samples is already saved
    if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:
      return
    
    # Get a minibatch of random samples from memory replay table
    minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)

    # Get current states from minibatch, then query NN model for Q values
    current_states = np.array([transition[0] for transition in minibatch])
    current_qs_list = self.model.predict(current_states)

    # Get future states from minibatch, then query NN model for Q values
    # When using target network, query it, otherwise main network should be queried
    new_current_states = np.array([transition[3] for transition in minibatch])
    future_qs_list = self.target_model.predict(new_current_states)
  
    X = []
    y = []

    # Now we need to enumerate our batches
    for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):

        # If not a terminal state, get new q from future states, otherwise set it to 0
        # almost like with Q Learning, but we use just part of equation here
        if not done:
            max_future_q = np.max(future_qs_list[index])
            new_q = reward + DISCOUNT * max_future_q
        else:
            new_q = reward

        # Update Q value for given state
        current_qs = current_qs_list[index]
        current_qs[action] = new_q

        # And append to our training data
        X.append(current_state)
        y.append(current_qs)

    # Fit on all samples as one batch, log only on terminal state
    self.model.fit(np.array(X), np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)

    # Update target network counter every episode
    if terminal_state:
        self.target_update_counter += 1

    # If counter reaches set value, update target network with weights of main network
    if self.target_update_counter > UPDATE_TARGET_EVERY:
        self.target_model.set_weights(self.model.get_weights())
        self.target_update_counter = 0
  
  # Queries main network for Q values given current observation space (environment state)
  def get_qs(self, state):
    return self.model.predict(np.array(state).reshape(-1, *state.shape))[0]

"""## Model Training

"""

# Environment Variables
EPISODES = 500_000

# Exploration settings
epsilon = 1  # not a constant, going to be decayed
EPSILON_DECAY = 0.999
MIN_EPSILON = 0.15

# Reward Settings
TWO_MULT = 1
THREE_MULT = 5
FOUR_MULT = 10
FIVE_MULT = 10000
OPPONENT_MULT = 1.1
mults = [TWO_MULT, THREE_MULT, FOUR_MULT, FIVE_MULT]

#  Stats settings
AGGREGATE_STATS_EVERY = 100  # episodes
SHOW_PREVIEW = False

# Save Model Settings
MODEL_CHECKPOINT_SIZE = 100
model_num = 0

agent = DQNAgent(model_weights='//home/kroohpar/csc487/models/BestGomoku__-99819.10max_-100079.13avg_-100880.30min__1646430535weights-1.h5')

ep_rewards = [0]
env.reset()
# Actual Training
for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):
    # Flag for if there is an error in this specific episode.
    exit_episode = False

    # Update tensorboard step every episode
    agent.tensorboard.step = episode

    # Restarting episode - reset episode reward and step number
    episode_reward = 0
    step = 1

    # Reset environment and get initial state
    current_state = env.reset()

    # Reset flag and start iterating until episode ends
    done = False
    while not done:

        # This part stays mostly the same, the change is to query a model for Q values
        if np.random.random() > epsilon:
            # Get action from Q table
            qs = agent.get_qs(current_state)
            action = find_max_Q(qs, env.action_space.valid_spaces)
        else:
            # Get random action
            action = np.random.choice(env.action_space.valid_spaces)

        try:
          new_state, _, done, info = env.step(action)
          reward = find_reward(env, env.player_color, mults, OPPONENT_MULT)
        except:
          exit_episode = True
        
        if exit_episode:
          break


        # Transform new continous state to new discrete state and count reward
        episode_reward += reward

        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:
            env.render()

        # Every step we update replay memory and train main network
        agent.update_replay_memory((current_state, action, reward, new_state, done))
        agent.train(done, step)

        current_state = new_state
        step += 1

    if not exit_episode:
      # Append episode reward to a list and log stats (every given number of episodes)
      ep_rewards.append(episode_reward)
      if not episode % AGGREGATE_STATS_EVERY or episode == 1:
          average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])
          min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])
          max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])
          print(f"Average reward last {AGGREGATE_STATS_EVERY} steps: {average_reward}")
          agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)

      # Save model, but only when min reward is greater or equal a set value
      if not episode % MODEL_CHECKPOINT_SIZE:
        agent.model.save_weights(f'/datasets/kroohpar/final_models/{MODEL_NAME+ str(model_num)}_{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__weights.h5')
        model_num += 1
        

      # Decay epsilon
      if epsilon > MIN_EPSILON:
          epsilon *= EPSILON_DECAY
          epsilon = max(MIN_EPSILON, epsilon)

